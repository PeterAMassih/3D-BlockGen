{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionModel3D(\n",
      "  (model): UNet3DConditionModel(\n",
      "    (conv_in): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (time_proj): Timesteps()\n",
      "    (time_embedding): TimestepEmbedding(\n",
      "      (linear_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (act): SiLU()\n",
      "      (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (transformer_in): TransformerTemporalModel(\n",
      "      (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "      (proj_in): Linear(in_features=64, out_features=512, bias=True)\n",
      "      (transformer_blocks): ModuleList(\n",
      "        (0): BasicTransformerBlock(\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn2): Attention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GEGLU(\n",
      "                (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (proj_out): Linear(in_features=512, out_features=64, bias=True)\n",
      "    )\n",
      "    (down_blocks): ModuleList(\n",
      "      (0): CrossAttnDownBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CrossAttnDownBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CrossAttnDownBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): DownBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (up_blocks): ModuleList(\n",
      "      (0): UpBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (2): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): CrossAttnUpBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (2): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(256, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (2): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (2): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): CrossAttnUpBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (2): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(128, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (2): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (2): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=128, out_features=1024, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): CrossAttnUpBlock3D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (temp_convs): ModuleList(\n",
      "          (0): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (1): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "          (2): TemporalConvLayer(\n",
      "            (conv1): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv2): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv3): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "            (conv4): Sequential(\n",
      "              (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "              (1): SiLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Conv3d(64, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (attentions): ModuleList(\n",
      "          (0): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): Transformer2DModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=512, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (temp_attentions): ModuleList(\n",
      "          (0): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (1): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (2): TransformerTemporalModel(\n",
      "            (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "            (proj_in): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (transformer_blocks): ModuleList(\n",
      "              (0): BasicTransformerBlock(\n",
      "                (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn1): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn2): Attention(\n",
      "                  (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "                  (to_out): ModuleList(\n",
      "                    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (ff): FeedForward(\n",
      "                  (net): ModuleList(\n",
      "                    (0): GEGLU(\n",
      "                      (proj): Linear(in_features=64, out_features=512, bias=True)\n",
      "                    )\n",
      "                    (1): Dropout(p=0.0, inplace=False)\n",
      "                    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (proj_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock3DCrossAttn(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (temp_convs): ModuleList(\n",
      "        (0): TemporalConvLayer(\n",
      "          (conv1): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv3): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv4): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "        )\n",
      "        (1): TemporalConvLayer(\n",
      "          (conv1): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv3): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "          (conv4): Sequential(\n",
      "            (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Conv3d(512, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (attentions): ModuleList(\n",
      "        (0): Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (temp_attentions): ModuleList(\n",
      "        (0): TransformerTemporalModel(\n",
      "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total parameters: 120096961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PeterAM/Desktop/Research_Project/3D-BlockGen/diffusion.py:12: FutureWarning: Accessing config attribute `cross_attention_dim` directly via 'UNet3DConditionModel' object attribute is deprecated. Please access 'cross_attention_dim' over 'UNet3DConditionModel's config object instead, e.g. 'unet.config.cross_attention_dim'.\n",
      "  self.dummy_encoder = nn.Parameter(torch.randn(1, 1, model.cross_attention_dim))\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet3DConditionModel\n",
    "from diffusion import DiffusionModel3D\n",
    "\n",
    "# Create a 3D model\n",
    "model = UNet3DConditionModel(\n",
    "    sample_size=32,  # the target resolution (assuming 32x32x32 voxels)\n",
    "    in_channels=1,  # number of input channels, 1 for voxel data\n",
    "    out_channels=1,  # number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 256, 512),  # channel numbers for each block\n",
    "    down_block_types=(\n",
    "        \"CrossAttnDownBlock3D\",  # a downsampling block with cross-attention\n",
    "        \"CrossAttnDownBlock3D\",\n",
    "        \"CrossAttnDownBlock3D\",\n",
    "        \"DownBlock3D\",  # a regular downsampling block without attention\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock3D\",  # a regular upsampling block without attention\n",
    "        \"CrossAttnUpBlock3D\",  # an upsampling block with cross-attention\n",
    "        \"CrossAttnUpBlock3D\",\n",
    "        \"CrossAttnUpBlock3D\",\n",
    "    ),\n",
    "    cross_attention_dim=512,  # dimension of the cross attention features\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "diffusion_model = DiffusionModel3D(unet3d_model, num_timesteps=1000)\n",
    "diffusion_model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(diffusion_model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in diffusion_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./objaverse_processed/processed_models...\n",
      "Found 1000 files in ./objaverse_processed/processed_models\n",
      "Train dataset size: 900\n",
      "Test dataset size: 100\n"
     ]
    }
   ],
   "source": [
    "# Import your data loader (assumed to be defined in a separate file)\n",
    "from data_loader import create_dataloader\n",
    "\n",
    "# Create training and testing dataloaders\n",
    "batch_size = 3  # Adjust as necessary for memory\n",
    "data_dir = \"./objaverse_processed/processed_models\"  # Replace with actual data directory\n",
    "train_dataloader, test_dataloader = create_dataloader(data_dir, batch_size=batch_size)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataloader.dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training:  14%|        | 42/300 [43:25<4:24:04, 61.41s/it]"
     ]
    }
   ],
   "source": [
    "from training import train_diffusion_model\n",
    "\n",
    "losses, test_losses = train_diffusion_model(diffusion_model, train_dataloader, test_dataloader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from inference import DiffusionInference3D\n",
    "\n",
    "inference = DiffusionInference3D(diffusion_model, diffusion_model.noise_scheduler, device=device)\n",
    "samples = inference.sample(num_samples=4, image_size=(32, 32, 32))\n",
    "\n",
    "inference.visualize_samples(samples, threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
